{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes: \n",
    "> + [Web Scraping](https://automatetheboringstuff.com/chapter11/)\n",
    "> + [JSON](http://docs.python-guide.org/en/latest/scenarios/json/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modules that make it easy to scrape web pages in Python.\n",
    "# (1) webbrowser: comes with Python and opens a browser to a specific page.\n",
    "# (2) Requests: downloads files and web pages from the Internet.\n",
    "#     Requests is easier and faster to use then urllib2.\n",
    "# (3) Beautiful Soup: parses HTML, the format that web pages are written in.\n",
    "# (4) Selenium: launches and controls a web browser. \n",
    "#     Selenium is able to fill in forms and simulate mouse clicks in this browser.\n",
    "# (5) pyperclip: A cross-platform clipboard module for Python. (only handles plain text for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open a web page in a browser using webbrowser\n",
    "import webbrowser\n",
    "webbrowser.open('http://inventwithpython.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: address: 870 Valencia St, San Francisco, CA 94110\n"
     ]
    }
   ],
   "source": [
    "# open a web page in a browser using webbrowser with input as either:\n",
    "# (1) *args [list of arguments] or\n",
    "# (2) **kwargs [list of key,value pairs] or\n",
    "# (3) sys.argv [only works as an app] or\n",
    "# (4) clipboard\n",
    "import webbrowser, sys, pyperclip \n",
    "\n",
    "def map_it(*args, **kwargs):\n",
    "    if (len(args)>0):\n",
    "        address = ''.join(args[:])\n",
    "        print('args: address: {}'.format(address))\n",
    "    elif (len(kwargs)>0):\n",
    "        values = []\n",
    "        for k,v in kwargs.items():\n",
    "            values.append(v)\n",
    "        address = ''.join(values)\n",
    "        print('kwargs: address: {}'.format(address))\n",
    "    #elif (len(sys.argv)>1):\n",
    "        #address = ' '.join(sys.argv[1:])\n",
    "        #print('argv: address: {}'.format(address))\n",
    "    else:\n",
    "        address = pyperclip.paste()\n",
    "        print('pyperclip: address: {}'.format(address))\n",
    "    webbrowser.open('https://www.google.com/maps/place/' + address)\n",
    "\n",
    "# test case - *args\n",
    "map_it('870 Valencia St,',' San Francisco,',' CA 94110')    \n",
    "\n",
    "# test case - **kwargs\n",
    "#map_it(add1='870 Valencia St,',add2=' San Francisco,',add3=' CA 94110') \n",
    "\n",
    "# test case - sys.argv\n",
    "# can do as an app, not in jupyter notebook\n",
    "\n",
    "# test case - clipboard\n",
    "#map_it()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping for 2 before opening https://automatetheboringstuff.com/\n",
      "sleeping for 2 before opening https://www.nostarch.com/automatestuff\n"
     ]
    }
   ],
   "source": [
    "# opens links on a web page in individual tabs\n",
    "# inputs:\n",
    "# (1) web page link\n",
    "# (2) max number of links on the web page to open in tabs\n",
    "# (3) sleep time between opening links to give browser to finish loading\n",
    "#     otherwise if window is not loaded completely, tabs could not be\n",
    "#     opened, and a new window is initiated for the next link\n",
    "\n",
    "from lxml import html\n",
    "import requests, webbrowser, time\n",
    "\n",
    "def open_tab(url, sleep_time):\n",
    "    print('sleeping for {} before opening {}'.format(sleep_time, url))\n",
    "    time.sleep(sleep_time)\n",
    "    webbrowser.open_new_tab(url)\n",
    "    \n",
    "def open_page_links_in_browser_tabs(url, max_page_urls_to_open, sleep_time):\n",
    "    page = requests.get(url)\n",
    "    tree = html.fromstring(page.content)\n",
    "    hrefs = tree.xpath('//a/@href')\n",
    "    hrefs_non_relative = []\n",
    "    for href in hrefs:\n",
    "        if 'http' in href:\n",
    "            hrefs_non_relative.append(href)\n",
    "    for href in hrefs_non_relative[:max_page_urls_to_open]:\n",
    "        open_tab(href,sleep_time)\n",
    "\n",
    "open_page_links_in_browser_tabs('https://automatetheboringstuff.com/',2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# opens a browser with local weather\n",
    "# inputs:\n",
    "# (1) zip code\n",
    "# (2) None - uses https://ipinfo.io/ to get local zip code\n",
    "\n",
    "import requests, json, webbrowser\n",
    "\n",
    "def open_weather(*zip):\n",
    "    if (len(zip)>0):\n",
    "        zip_code = zip\n",
    "    else:\n",
    "        res = requests.get('https://ipinfo.io')\n",
    "        res_json = json.loads(res.text[:])\n",
    "        zip_code = res_json['postal']\n",
    "        \n",
    "        # can use google in chrome to get local zip code\n",
    "        # needs web scraping through xpath\n",
    "        #res = requests.get('http://www.google.com/search?q=local zip code')\n",
    "        #tree = html.fromstring(res.content)\n",
    "        #divs = tree.xpath('//b/text()')\n",
    "        #print(divs)\n",
    "\n",
    "    webbrowser.open('https://weather.com/weather/today/l/'+zip_code)\n",
    "\n",
    "# test case - zip code\n",
    "#open_weather('06902')\n",
    "\n",
    "# test case - none\n",
    "open_weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response type: <class 'requests.models.Response'>\n",
      "response status ok? True\n",
      "response content size: 174130 characters\n",
      "\n",
      "response content (first 250 characters):\n",
      "ï»¿The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project\n",
      "\n",
      "There was a problem: 404 Client Error: Not Found for url: http://inventwithpython.com/page_that_does_not_exist\n",
      "\n",
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# response object\n",
    "res = requests.get('https://automatetheboringstuff.com/files/rj.txt')\n",
    "print('response type: {}'.format(type(res)))\n",
    "print('response status ok? {}'.format(res.status_code == requests.codes.ok))\n",
    "print('response content size: {} characters'.format(len(res.text)))\n",
    "print('\\nresponse content (first 250 characters):\\n{}'.format(res.text[:250]))\n",
    "\n",
    "# exception handling\n",
    "res = requests.get('http://inventwithpython.com/page_that_does_not_exist')\n",
    "try:\n",
    "    res.raise_for_status()\n",
    "except Exception as exc:\n",
    "    print('\\nThere was a problem: {}'.format(exc))\n",
    "\n",
    "# saving content\n",
    "res = requests.get('https://automatetheboringstuff.com/files/rj.txt')\n",
    "try:\n",
    "    res.raise_for_status()\n",
    "    # wb=write binary even though content is text\n",
    "    playFile = open('RomeoAndJuliet.txt', 'wb') \n",
    "    for chunk in res.iter_content(100000):\n",
    "        playFile.write(chunk)\n",
    "    playFile.close()\n",
    "    print('\\nSaved.')\n",
    "except Exception as exc:\n",
    "    print('\\nThere was a problem: {}'.format(exc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "res = requests.get('http://nostarch.com')\n",
    "res.raise_for_status()\n",
    "# lxml to get rid of parser not specified warning\n",
    "noStarchSoup = BeautifulSoup(res.text,\"lxml\") \n",
    "type(noStarchSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selector passed to the select() method -- Will match\n",
    "# soup.select('div') -- All elements named <div>\n",
    "# soup.select('#author') -- The element with an id attribute of author\n",
    "# soup.select('.notice') -- All elements that use a CSS class attribute named notice\n",
    "# soup.select('div span') -- All elements named <span> that are within an element named <div>\n",
    "# soup.select('div > span') -- All elements named <span> that are directly within an element \n",
    "#                              named <div>, with no other element in between\n",
    "# soup.select('input[name]') -- All elements named <input> that have a name attribute with any value\n",
    "# soup.select('input[type=\"button\"]') -- All elements named <input> that have an attribute named \n",
    "#                                        type with value button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the example file from  http://nostarch.com/automatestuff/\n",
    "exampleFile = open('example.html')\n",
    "exampleSoup = BeautifulSoup(exampleFile,\"lxml\")\n",
    "type(exampleSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type elems: <class 'list'>\n",
      "len elems: 1\n",
      "type elems[0]: <class 'bs4.element.Tag'>\n",
      "elems[0].getText(): Al Sweigart\n",
      "str(elems[0]): <span id=\"author\">Al Sweigart</span>\n",
      "elems[0].attrs: {'id': 'author'}\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "exampleFile = open('example.html')\n",
    "exampleSoup = bs4.BeautifulSoup(exampleFile.read(),\"lxml\")\n",
    "elems = exampleSoup.select('#author')\n",
    "print('type elems: {}'.format(type(elems)))\n",
    "print('len elems: {}'.format(len(elems)))\n",
    "print('type elems[0]: {}'.format(type(elems[0])))\n",
    "print('elems[0].getText(): {}'.format(elems[0].getText()))\n",
    "print('str(elems[0]): {}'.format(str(elems[0])))\n",
    "print('elems[0].attrs: {}'.format(elems[0].attrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
