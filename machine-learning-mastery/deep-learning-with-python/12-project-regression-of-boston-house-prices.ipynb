{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from run_notebook import execute_notebook\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Shape:\n",
      "(506, 14)\n",
      "====================================================================================================\n",
      "_array:\n",
      "[[  6.32000000e-03   1.80000000e+01   2.31000000e+00   0.00000000e+00\n",
      "    5.38000000e-01   6.57500000e+00   6.52000000e+01   4.09000000e+00\n",
      "    1.00000000e+00   2.96000000e+02   1.53000000e+01   3.96900000e+02\n",
      "    4.98000000e+00   2.40000000e+01]\n",
      " [  2.73100000e-02   0.00000000e+00   7.07000000e+00   0.00000000e+00\n",
      "    4.69000000e-01   6.42100000e+00   7.89000000e+01   4.96710000e+00\n",
      "    2.00000000e+00   2.42000000e+02   1.78000000e+01   3.96900000e+02\n",
      "    9.14000000e+00   2.16000000e+01]\n",
      " [  2.72900000e-02   0.00000000e+00   7.07000000e+00   0.00000000e+00\n",
      "    4.69000000e-01   7.18500000e+00   6.11000000e+01   4.96710000e+00\n",
      "    2.00000000e+00   2.42000000e+02   1.78000000e+01   3.92830000e+02\n",
      "    4.03000000e+00   3.47000000e+01]\n",
      " [  3.23700000e-02   0.00000000e+00   2.18000000e+00   0.00000000e+00\n",
      "    4.58000000e-01   6.99800000e+00   4.58000000e+01   6.06220000e+00\n",
      "    3.00000000e+00   2.22000000e+02   1.87000000e+01   3.94630000e+02\n",
      "    2.94000000e+00   3.34000000e+01]\n",
      " [  6.90500000e-02   0.00000000e+00   2.18000000e+00   0.00000000e+00\n",
      "    4.58000000e-01   7.14700000e+00   5.42000000e+01   6.06220000e+00\n",
      "    3.00000000e+00   2.22000000e+02   1.87000000e+01   3.96900000e+02\n",
      "    5.33000000e+00   3.62000000e+01]]\n",
      "====================================================================================================\n",
      "_X\n",
      "[[  6.32000000e-03   1.80000000e+01   2.31000000e+00   0.00000000e+00\n",
      "    5.38000000e-01   6.57500000e+00   6.52000000e+01   4.09000000e+00\n",
      "    1.00000000e+00   2.96000000e+02   1.53000000e+01   3.96900000e+02\n",
      "    4.98000000e+00]\n",
      " [  2.73100000e-02   0.00000000e+00   7.07000000e+00   0.00000000e+00\n",
      "    4.69000000e-01   6.42100000e+00   7.89000000e+01   4.96710000e+00\n",
      "    2.00000000e+00   2.42000000e+02   1.78000000e+01   3.96900000e+02\n",
      "    9.14000000e+00]\n",
      " [  2.72900000e-02   0.00000000e+00   7.07000000e+00   0.00000000e+00\n",
      "    4.69000000e-01   7.18500000e+00   6.11000000e+01   4.96710000e+00\n",
      "    2.00000000e+00   2.42000000e+02   1.78000000e+01   3.92830000e+02\n",
      "    4.03000000e+00]\n",
      " [  3.23700000e-02   0.00000000e+00   2.18000000e+00   0.00000000e+00\n",
      "    4.58000000e-01   6.99800000e+00   4.58000000e+01   6.06220000e+00\n",
      "    3.00000000e+00   2.22000000e+02   1.87000000e+01   3.94630000e+02\n",
      "    2.94000000e+00]\n",
      " [  6.90500000e-02   0.00000000e+00   2.18000000e+00   0.00000000e+00\n",
      "    4.58000000e-01   7.14700000e+00   5.42000000e+01   6.06220000e+00\n",
      "    3.00000000e+00   2.22000000e+02   1.87000000e+01   3.96900000e+02\n",
      "    5.33000000e+00]]\n",
      "====================================================================================================\n",
      "_Y\n",
      "[[ 24. ]\n",
      " [ 21.6]\n",
      " [ 34.7]\n",
      " [ 33.4]\n",
      " [ 36.2]]\n",
      "====================================================================================================\n",
      "_Y raveled\n",
      "[ 24.   21.6  34.7  33.4  36.2]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "execute_notebook('./data/dataset-boston-house-prices.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    _model = Sequential()\n",
    "    _model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    _model.add(Dense(1, kernel_initializer='normal'))\n",
    "    _model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return _model\n",
    "\n",
    "def create_deeper():\n",
    "    _model = Sequential()\n",
    "    _model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    _model.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "    _model.add(Dense(1, kernel_initializer='normal'))\n",
    "    _model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return _model    \n",
    "\n",
    "def create_wider():\n",
    "    _model = Sequential()\n",
    "    _model.add(Dense(20, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    _model.add(Dense(1, kernel_initializer='normal'))\n",
    "    _model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_estimator_baseline = KerasRegressor(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_kfold = KFold(n_splits=10, random_state=_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.66 minutes\n"
     ]
    }
   ],
   "source": [
    "_start = time()\n",
    "_results = cross_val_score(_estimator_baseline, _X, _Y, cv=_kfold)\n",
    "_end = time()\n",
    "print('time: {:.2f} minutes'.format((_end-_start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 31.909550037617777 (21.72701182706889) MSE\n"
     ]
    }
   ],
   "source": [
    "print('Baseline: {} ({}) MSE'.format(_results.mean(), _results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An important concern with the Boston house price dataset is that the input attributes all vary in their \n",
    "# scales because they measure different quantities. \n",
    "# We can use scikit-learnâ€™s Pipeline framework3 to perform the standardization during the model evaluation \n",
    "# process, within each fold of the cross-validation. \n",
    "# This ensures that there is no data leakage from each testset cross-validation fold into the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_estimators = []\n",
    "_estimators.append(('Standardize', StandardScaler()))\n",
    "_estimators.append(('mlp', KerasRegressor(build_fn=create_baseline, epochs=100, batch_size=10, verbose=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_pipeline = Pipeline(_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.91 minutes\n"
     ]
    }
   ],
   "source": [
    "_start = time()\n",
    "_results = cross_val_score(_pipeline, _X, _Y, cv=_kfold)\n",
    "_end = time()\n",
    "print('time: {:.2f} minutes'.format((_end-_start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 27.378453547025423 (30.618107830217973) MSE\n"
     ]
    }
   ],
   "source": [
    "print('Baseline: {} ({}) MSE'.format(_results.mean(), _results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A further extension of this section would be to similarly apply a rescaling to the output variable such \n",
    "# as normalizing it to the range of 0 to 1 and use a Sigmoid or similar activation function on the output \n",
    "# layer to narrow output predictions to the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate a deeper network topology:\n",
    "\n",
    "# One way to improve the performance of a neural network is to add more layers. \n",
    "# This might allow the model to extract and recombine higher order features embedded in the data. \n",
    "# In this section we will evaluate the effect of adding one more hidden layer to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_estimators_deeper = []\n",
    "_estimators_deeper.append(('Standardize', StandardScaler()))\n",
    "_estimators_deeper.append(('mlp', KerasRegressor(build_fn=create_deeper, epochs=100, batch_size=10, verbose=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_pipeline_deeper = Pipeline(_estimators_deeper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.10 minutes\n"
     ]
    }
   ],
   "source": [
    "_start = time()\n",
    "_results = cross_val_score(_pipeline_deeper, _X, _Y, cv=_kfold)\n",
    "_end = time()\n",
    "print('time: {:.2f} minutes'.format((_end-_start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 21.368724352470508 (25.9216013776058) MSE\n"
     ]
    }
   ],
   "source": [
    "print('Baseline: {} ({}) MSE'.format(_results.mean(), _results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate a wider network topology\n",
    "\n",
    "# In this section we evaluate the effect of keeping a shallow network architecture and nearly doubling the \n",
    "# number of neurons in the one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_estimators_wider = []\n",
    "_estimators_wider.append(('Standardize', StandardScaler()))\n",
    "_estimators_wider.append(('mlp', KerasRegressor(build_fn=create_wider, epochs=100, batch_size=10, verbose=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_pipeline_wider = Pipeline(_estimators_wider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.00 minutes\n"
     ]
    }
   ],
   "source": [
    "_start = time()\n",
    "_results = cross_val_score(_pipeline_wider, _X, _Y, cv=_kfold)\n",
    "_end = time()\n",
    "print('time: {:.2f} minutes'.format((_end-_start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 24.636608265607673 (27.555178708535756) MSE\n"
     ]
    }
   ],
   "source": [
    "print('Baseline: {} ({}) MSE'.format(_results.mean(), _results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The results demonstrate the importance of empirical testing when it comes to developing neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
