{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * [Code Notebook](https://github.com/ageron/handson-ml/blob/master/01_the_machine_learning_landscape.ipynb)\n",
    "\n",
    "## Introduction ##\n",
    "* This chapter introduces a lot of fundamental concepts (and jargon) that every data scientist should know by heart. It will be a high-level overview (the only chapter without much code), all rather simple, but you should make sure everything is crystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s get started!\n",
    "\n",
    "## What is Machine Learning ##\n",
    "\n",
    "* Machine Learning is the science (and art) of programming computers so they can learn from data.\n",
    "* Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed. (Arthur Samuel, 1959)\n",
    "* A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. (Tom Mitchell, 1997)\n",
    "\n",
    "> **Example:** Spam Filter.\n",
    ">\n",
    "> **Task:** Correctly identify spam.\n",
    ">\n",
    "> **Experience:** Training data (collection of emails) + algorithms (machine learning) = model\n",
    ">\n",
    "> **Performance:** Ratio of correctly classified emails as spam.\n",
    "\n",
    "## Why Use Machine Learning? ##\n",
    "\n",
    "Machine Learning is great for:\n",
    "\n",
    "* Problems for which existing solutions require a lot of hand-tuning or long lists of rules: one Machine Learning algorithm can often simplify code and perform better.\n",
    "    > * For example: Spam Filter.\n",
    "    > * Traditional approach: Write a rule based system ('4U', 'cre%it car%'). But as spammers become smarter (using 'ForU' instead of '4U'), the rules list will keep growing and quickly get unweildy.\n",
    "    > * Machine learning approach will keep learning as new training data is fed to the algorithm.\n",
    "* Complex problems for which there is no good solution at all using a traditional approach: the best Machine Learning techniques can find a solution.\n",
    "    > * For example: Speech recognition for words 'one' & 'two'.\n",
    "    > * Traditional rule based approach will be very difficult to even define a good solution.\n",
    "    > * Machine learning approach only needs a good training set of recordings of the words.\n",
    "* Fluctuating environments: a Machine Learning system can adapt to new data.\n",
    "* Getting insights about complex problems and large amounts of data.\n",
    "    > * For example: Data mining.\n",
    "    > * Inspect the ML based solution.\n",
    "    > * Understand the problem better.\n",
    "    > * Iterate if needed.\n",
    "\n",
    "## Types of Machine Learning Systems ##\n",
    "\n",
    "1. Trained with/without human supervision:\n",
    "    > * Supervised\n",
    "    > * Unsupervised\n",
    "    > * Semisupervised\n",
    "    > * Reinforcement learning\n",
    "2. Can/cannot learn incrementally on the fly:\n",
    "    > * Online learning\n",
    "    > * Batch learning\n",
    "3. Whether they work by simply comparing new data points to known data points, or instead detect patterns in the training data and build a predictive model, much like scientists do:                   \n",
    "    > * Instance-based learning\n",
    "    > * Model-based learning\n",
    "4. Example:\n",
    "    > * A state-of-the-art spam filter may learn on the fly using a deep neural network model trained using examples of spam and ham; this makes it an online, model-based, supervised learning system.\n",
    "    \n",
    "### Supervised Learning ###\n",
    "\n",
    "* The training data fed to the algorithm includes the desired solutions, called *labels*.\n",
    "* The training data features/attribute are called *predictors*.\n",
    "* Two types:\n",
    "    1. Classification\n",
    "        > * Training data labels are non-numeric.\n",
    "        > * Example: Spam filter (spam or ham)\n",
    "    2. Regression\n",
    "        > * Training data labels are numeric.\n",
    "        > * Example: Car prices\n",
    "        > * Note that some regression algorithms can be used for classification as well, and vice versa. For example, Logistic Regression is commonly used for classification, as it can output a value that corresponds to the probability of belonging to a given class (e.g., 20% chance of being spam).\n",
    "* Important supervised learning algorithms:\n",
    "    > * k-Nearest Neighbors\n",
    "    > * Linear Regression\n",
    "    > * Logistic Regression\n",
    "    > * Support Vector Machines (SVMs)\n",
    "    > * Decision Trees and Random Forests\n",
    "    > * Neural networks (Some neural network architectures can be unsupervised, such as autoencoders and restricted Boltzmann machines. They can also be semisupervised, such as in deep belief networks and unsupervised pretraining.)\n",
    "    \n",
    " \n",
    "> **_NOTE_**\n",
    "> In Machine Learning an attribute is a data type (e.g., “Mileage”), while a feature has several meanings depending on the context, but generally means an attribute plus its value (e.g., “Mileage = 15,000”). Many people use the words attribute and feature interchangeably, though.\n",
    "\n",
    "### Unsupervised Learning ###\n",
    "\n",
    "* The training data does not contain the desired solutions and hence is *unlabeled*.\n",
    "* Unsupervised learning tasks:\n",
    "    > * **Clustering** - detect groups/sub-groups (HCA) within data.\n",
    "    > * **Visualization** - visualize (2D/3D) clusters in the input space (t-SNE)\n",
    "    > * **Dimensionality Reduction** - simplify the data without losing too much information. One way to do this is to merge several correlated features into one. For example, a car’s mileage may be very correlated with its age, so the dimensionality reduction algorithm will merge them into one feature that represents the car’s wear and tear. This is called **_feature extraction_**. \n",
    "    > * **TIP:** _It is often a good idea to try to reduce the dimension of your training data using a dimensionality reduction algorithm before you feed it to another Machine Learning algorithm (such as a supervised learning algorithm). It will run much faster, the data will take up less disk and memory space, and in some cases it may also perform better._\n",
    "    > * **Anomaly Detection** - detecting unusual credit card transactions to prevent fraud, catching manufacturing defects, or automatically removing outliers from a dataset before feeding it to another learning algorithm. The system is trained with normal instances, and when it sees a new instance it can tell whether it looks like a normal one or whether it is likely an anomaly.\n",
    "    > * **Association Rule Learning** - dig into large amounts of data and discover interesting relations between attributes. For example, suppose you own a supermarket. Running an association rule on your sales logs may reveal that people who purchase barbecue sauce and potato chips also tend to buy steak. Thus, you may want to place these items close to each other.\n",
    "* Important unsupervised learning algorithms:\n",
    "    > * Clustering\n",
    "    > * k-Means\n",
    "    > * Hierarchical Cluster Analysis (HCA)\n",
    "    > * Expectation Maximization\n",
    "    > * Visualization and dimensionality reduction\n",
    "    > * Principal Component Analysis (PCA)\n",
    "    > * Kernel PCA\n",
    "    > * Locally-Linear Embedding (LLE)\n",
    "    > * t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "    > * Association rule learning\n",
    "    > * Apriori\n",
    "    > * Eclat\n",
    "\n",
    "### Semisupervised Learning ###\n",
    "\n",
    "* The training data has little bit of labeled data and a lot of unlabeled data.\n",
    "* Example:\n",
    "    > Some photo-hosting services, such as Google Photos, are good examples of this. Once you upload all your family photos to the service, it automatically recognizes that the same person A shows up in photos 1, 5, and 11, while another person B shows up in photos 2, 5, and 7. This is the unsupervised part of the algorithm (clustering). Now all the system needs is for you to tell it who these people are. Just one label per person and it is able to name everyone in every photo, which is useful for searching photos.\n",
    "* Algorithms:\n",
    "    > Most semisupervised learning algorithms are combinations of unsupervised and supervised algorithms. For example, deep belief networks (DBNs) are based on unsupervised components called restricted Boltzmann machines (RBMs) stacked on top of one another. RBMs are trained sequentially in an unsupervised manner, and then the whole system is fine-tuned using supervised learning techniques.\n",
    "    \n",
    "### Reinforcement Learning ###\n",
    "\n",
    "* The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards. It must then learn by itself what is the best strategy, called a policy, to get the most reward over time. A policy defines what action the agent should choose when it is in a given situation.\n",
    "* Examples:\n",
    "    > * Robots learning how to walk.\n",
    "    > * DeepMind’s AlphaGo learned its winning policy by analyzing millions of games, and then playing many games against itself.\n",
    "    \n",
    "### Batch Learning (aka Offline Learning) ###\n",
    "\n",
    "* System needs to be trained on all the available data.\n",
    "* It can consume a lot of time and computing resources based on the amount of data.\n",
    "* Hence, the learning needs to be done offline usually.\n",
    "* A schedule (daily or weekly) can be setup for re-learning using new plus existing data.\n",
    "\n",
    "### Online Learning ###\n",
    "\n",
    "* System can be trained incrementally with individual instances or mini-batches of instances.\n",
    "* Good when:\n",
    "    > * Data is received in a continous flow (example: stock prices).\n",
    "    > * Computing resources are limited - learned instances can be discarded.\n",
    "    > * Huge datasets that cannot fit in one machine's memory.  The algorithm loads part of the data, trains on it and repeats the process until all data is run. This is called **_out-of-core learning_**.\n",
    "* **WARNING:** _This whole process is usually done offline (i.e., not on the live system), so **online learning** can be a confusing name. Think of it as **incremental learning** _.\n",
    "* **Learning Rate** - measure of how fast the system adapts to changing data. Too high and the system only remembers the latest training. Too low and system learns from new data slowly. Medium might be the way to go.\n",
    "* System needs to be monitored very closely to react accordingly in time to abnormal or bad data.\n",
    "\n",
    "### Instance-based Learning ###\n",
    "\n",
    "* System learns the examples by heart, then generalizes to new cases using a similarity measure. \n",
    "* Example: \n",
    "    > * Learn by heart a group of spam emails, then use them to establish similarity measure for new emails by counting the number of words they have in common.\n",
    "    \n",
    "### Model-based Learning ###\n",
    "\n",
    "* System learns by making a model out of training instances, then uses that model to make predictions on new instances.\n",
    "* Use data from [OECD - Better Life Index](http://stats.oecd.org/index.aspx?DataSetCode=BLI) & [IMF - GDP per capita](http://www.imf.org/external/pubs/ft/weo/2016/01/weodata/weorept.aspx?pr.x=32&pr.y=8&sy=2015&ey=2015&scsm=1&ssd=1&sort=country&ds=.&br=1&c=512%2C668%2C914%2C672%2C612%2C946%2C614%2C137%2C311%2C962%2C213%2C674%2C911%2C676%2C193%2C548%2C122%2C556%2C912%2C678%2C313%2C181%2C419%2C867%2C513%2C682%2C316%2C684%2C913%2C273%2C124%2C868%2C339%2C921%2C638%2C948%2C514%2C943%2C218%2C686%2C963%2C688%2C616%2C518%2C223%2C728%2C516%2C558%2C918%2C138%2C748%2C196%2C618%2C278%2C624%2C692%2C522%2C694%2C622%2C142%2C156%2C449%2C626%2C564%2C628%2C565%2C228%2C283%2C924%2C853%2C233%2C288%2C632%2C293%2C636%2C566%2C634%2C964%2C238%2C182%2C662%2C453%2C960%2C968%2C423%2C922%2C935%2C714%2C128%2C862%2C611%2C135%2C321%2C716%2C243%2C456%2C248%2C722%2C469%2C942%2C253%2C718%2C642%2C724%2C643%2C576%2C939%2C936%2C644%2C961%2C819%2C813%2C172%2C199%2C132%2C733%2C646%2C184%2C648%2C524%2C915%2C361%2C134%2C362%2C652%2C364%2C174%2C732%2C328%2C366%2C258%2C734%2C656%2C144%2C654%2C146%2C336%2C463%2C263%2C528%2C268%2C923%2C532%2C738%2C944%2C578%2C176%2C537%2C534%2C742%2C536%2C866%2C429%2C369%2C433%2C744%2C178%2C186%2C436%2C925%2C136%2C869%2C343%2C746%2C158%2C926%2C439%2C466%2C916%2C112%2C664%2C111%2C826%2C298%2C542%2C927%2C967%2C846%2C443%2C299%2C917%2C582%2C544%2C474%2C941%2C754%2C446%2C698%2C666&s=NGDPDPC&grp=0&a=) against linear regression algorithm to create a model to predict life satisfaction for countries not included in training data given their GDP per capita.\n",
    "    > * life_satisfaction = x0 + x1 * gdp_per_capita\n",
    "    \n",
    "### Main Challenges of Machine Learning ###\n",
    "\n",
    "1. Insufficient Quantity of Training Data\n",
    "    > * Even for very simple problems you typically need thousands of examples, and for complex problems such as image or speech recognition you may need millions of examples.\n",
    "    > * **THE UNREASONABLE EFFECTIVENESS OF DATA** In a famous paper published in 2001, Microsoft researchers Michele Banko and Eric Brill showed that very different Machine Learning algorithms, including fairly simple ones, performed almost identically well on a complex problem of natural language disambiguation8 once they were given enough data. The idea that data matters more than algorithms for complex problems was further popularized by Peter Norvig et al. in a paper titled “The Unreasonable Effectiveness of Data” published in 2009.\n",
    "2. Nonrepresentative Training Data\n",
    "    > * In order to generalize well, it is crucial that your training data be representative of the new cases you want to generalize to. This is true whether you use instance-based learning or model-based learning.\n",
    "    > * If the sample is too small, you will have **_sampling noise_** (i.e., nonrepresentative data as a result of chance)\n",
    "    > * Even very large samples can be nonrepresentative if the sampling method is flawed. This is called **_sampling bias_**.\n",
    "3. Poor-Quality Data\n",
    "    > *  If your training data is full of errors, outliers, and noise (e.g., due to poor-quality measurements), it will make it harder for the system to detect the underlying patterns, so your system is less likely to perform well. Poor quality data needs to be cleaned by deciding whether to ignore the outliers and missing instances or fill in the missing values or train one model with the feature and one model without it, and so on.\n",
    "4. Irrelevant Features\n",
    "    > * A system will only learn effectively if the training data contains enough relevant features and not too many irrelevant ones. Coming up with a good set of features to train on is called **_feature engineering_**, which involves:\n",
    "        * **_Feature selection:_** selecting the most useful features to train on among existing features.\n",
    "        * **_Feature extraction:_** combining existing features to produce a more useful one (dimensionality reduction algorithms can help).\n",
    "        * Creating new features by gathering new data.\n",
    "5. Overfitting the Training Data\n",
    "    > * Overfitting means that the model performs well on the training data, but it does not generalize well.\n",
    "    > * If the training set is noisy, or if it is too small (which introduces sampling noise), then the model is likely to detect patterns in the noise itself. Obviously these patterns will not generalize to new instances.    \n",
    "    > * **_WARNING_** Overfitting happens when the model is too complex relative to the amount and noisiness of the training data. The possible solutions are:\n",
    "        * To simplify the model by selecting one with fewer parameters (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data or by constraining the model\n",
    "        * To gather more training data\n",
    "        * To reduce the noise in the training data (e.g., fix data errors and remove outliers)\n",
    "    > * Constraining a model through manipulating it's parameters (x0 and x1 in linear regression algorithm which gives it **_two degrees of freedom_**) to make it simpler and reduce the risk of overfitting is called **_regularization_**.\n",
    "    > * The amount of regularization to apply during learning can be controlled by a **_hyperparameter_**. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. \n",
    "6. Underfitting the Training Data\n",
    "    > * Underfitting occurs when a model is too simple to learn the underlying structure of the data.\n",
    "    > * The main options to fix this problem are:\n",
    "        * Selecting a more powerful model, with more parameters.\n",
    "        * Feeding better features to the learning algorithm (feature engineering).\n",
    "        * Reducing the constraints on the model (e.g., reducing the regularization hyperparameter).\n",
    "        \n",
    "### Testing and Validating ###\n",
    "\n",
    "* You train multiple models with various hyperparameters using the **_training set_**, you select the model and hyperparameters that perform best on the **_validation set_**, and when you’re happy with your model you run a single final test against the **_test set_** to get an estimate of the **_generalized error (or out-of-sample error)_**.\n",
    "* To avoid “wasting” too much training data in validation sets, a common technique is to use cross-validation: the training set is split into complementary subsets, and each model is trained against a different combination of these subsets and validated against the remaining parts. Once the model type and hyperparameters have been selected, a final model is trained using these hyperparameters on the full training set, and the **_generalized error (or out-of-sample error)_** is measured on the test set.\n",
    "\n",
    "### NO FREE LUNCH THEOREM ###\n",
    "* A model is a simplified version of the observations. The simplifications are meant to discard the superfluous details that are unlikely to generalize to new instances. However, to decide what data to discard and what data to keep, you must make assumptions. For example, a linear model makes the assumption that the data is fundamentally linear and that the distance between the instances and the straight line is just noise, which can safely be ignored.\n",
    "* In a famous 1996 paper,11 David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. For some datasets the best model is a linear model, while for other datasets it is a neural network. There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and you evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks.\n",
    "\n",
    "### Exercises ###\n",
    "1. How would you define Machine Learning?\n",
    "2. Can you name four types of problems where it shines?\n",
    "3. What is a labeled training set?\n",
    "4. What are the two most common supervised tasks?\n",
    "5. Can you name four common unsupervised tasks?\n",
    "6. What type of Machine Learning algorithm would you use to allow a robot to walk in various unknown terrains?\n",
    "7. What type of algorithm would you use to segment your customers into multiple groups?\n",
    "8. Would you frame the problem of spam detection as a supervised learning problem or an unsupervised learning problem?\n",
    "9. What is an online learning system?\n",
    "10. What is out-of-core learning?\n",
    "11. What type of learning algorithm relies on a similarity measure to make predictions?\n",
    "12. What is the difference between a model parameter and a learning algorithm’s hyperparameter?\n",
    "13. What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions?\n",
    "14. Can you name four of the main challenges in Machine Learning?\n",
    "15. If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions?\n",
    "16. What is a test set and why would you want to use it?\n",
    "17. What is the purpose of a validation set?\n",
    "18. What can go wrong if you tune hyperparameters using the test set?\n",
    "19. What is cross-validation and why would you prefer it to a validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
