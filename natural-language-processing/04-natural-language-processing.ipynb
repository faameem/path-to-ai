{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes: [natural language processing](https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Corpus - Body of text. Corpora - plural.\n",
    "# Lexicon - Words and their meanings.\n",
    "# Token - Each \"entity\" that is a part of whatever was split up based on rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence tokens: ['Hello Mr. Smith, how are you doing today?', 'The weather is great,    and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n",
      "\n",
      "word tokens: ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "# sentence and word tokens\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def get_tokens():\n",
    "    example_text = \"Hello Mr. Smith, how are you doing today? The weather is great,\\\n",
    "    and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
    "    print('sentence tokens: {}'.format(sent_tokenize(example_text)))\n",
    "    print('\\nword tokens: {}'.format(word_tokenize(example_text)))\n",
    "\n",
    "get_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word tokens: ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "\n",
      "filtered word tokens: ['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "# word tokens without stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_tokens_sans_stopwords():\n",
    "    example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(example_sent)\n",
    "    filtered_word_tokens = [w for w in word_tokens if not w in stop_words]\n",
    "    print('word tokens: {}'.format(word_tokens))\n",
    "    print('\\nfiltered word tokens: {}'.format(filtered_word_tokens))\n",
    "\n",
    "get_tokens_sans_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example words: ['python', 'pythoner', 'pythoning', 'pythoned', 'pythonly']\n",
      "\n",
      "stemmed example words:\n",
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n",
      "\n",
      "new_text word tokens: ['It', 'is', 'important', 'to', 'by', 'very', 'pythonly', 'while', 'you', 'are', 'pythoning', 'with', 'python', '.', 'All', 'pythoners', 'have', 'pythoned', 'poorly', 'at', 'least', 'once', '.']\n",
      "\n",
      "stemmed new_text word tokens:\n",
      "It\n",
      "is\n",
      "import\n",
      "to\n",
      "by\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "All\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# word stemming\n",
    "\n",
    "# Many variations of words carry the same meaning, other than when tense is involved.\n",
    "# Stemming helps to shorten the lookup, and normalize sentences.\n",
    "# Consider:\n",
    "# I was taking a ride in the car.\n",
    "# I was riding in the car.\n",
    "# This sentence means the same thing. in the car is the same. I was is the same. \n",
    "# the ing denotes a clear past-tense in both cases, so is it truly necessary to differentiate \n",
    "# between ride and riding, in the case of just trying to figure out the meaning of what this \n",
    "# past-tense activity was?\n",
    "# No, not really.\n",
    "# Imagine every word in the English language, every possible tense and affix you can put on \n",
    "# a word. Having individual dictionary entries per version would be highly redundant and \n",
    "# inefficient, especially since, once we convert to numbers, the \"value\" is going to be \n",
    "# identical.\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def get_stem_words():\n",
    "    ps = PorterStemmer()\n",
    "    example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "    print('example words: {}'.format(example_words))\n",
    "    print('\\nstemmed example words:')\n",
    "    for w in example_words:\n",
    "        print(ps.stem(w))\n",
    "        \n",
    "    new_text = \"It is important to by very pythonly while you are pythoning with python. \\\n",
    "                All pythoners have pythoned poorly at least once.\"\n",
    "    words = word_tokenize(new_text)\n",
    "    print('\\nnew_text word tokens: {}'.format(words))\n",
    "    print('\\nstemmed new_text word tokens:')\n",
    "    for w in words:\n",
    "        print(ps.stem(w))\n",
    "\n",
    "get_stem_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# part of speech (pos) tagging\n",
    "\n",
    "\"\"\"\n",
    "POS tag list:\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent's\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when\n",
    "\"\"\"\n",
    "\n",
    "# PunktSentenceTokenizer is capable of unsupervised machine learning, so you can actually \n",
    "# train it on any body of text that you use. \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "\n",
    "\n",
    "def get_pos_tags():\n",
    "    train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "    sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "    custom_sent_tokenizer = PunktSentenceTokenizer(train_text) # train\n",
    "    tokenized = custom_sent_tokenizer.tokenize(sample_text) # test\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "get_pos_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "chunked: (S\n",
      "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "  'S/POS\n",
      "  (Chunk ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP UNION/NNP January/NNP)\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  (Chunk THE/NNP PRESIDENT/NNP)\n",
      "  :/:\n",
      "  (Chunk Thank/NNP)\n",
      "  you/PRP\n",
      "  all/DT\n",
      "  ./.)\n",
      "subtree: (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "subtree: (Chunk ADDRESS/NNP)\n",
      "subtree: (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "subtree: (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "subtree: (Chunk THE/NNP UNION/NNP January/NNP)\n",
      "subtree: (Chunk THE/NNP PRESIDENT/NNP)\n",
      "subtree: (Chunk Thank/NNP)\n",
      "1\n",
      "chunked: (S\n",
      "  (Chunk Mr./NNP Speaker/NNP)\n",
      "  ,/,\n",
      "  (Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  (Chunk Congress/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Supreme/NNP Court/NNP)\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:\n",
      "  Today/VB\n",
      "  our/PRP$\n",
      "  nation/NN\n",
      "  lost/VBD\n",
      "  a/DT\n",
      "  beloved/VBN\n",
      "  ,/,\n",
      "  graceful/JJ\n",
      "  ,/,\n",
      "  courageous/JJ\n",
      "  woman/NN\n",
      "  who/WP\n",
      "  (Chunk called/VBD America/NNP)\n",
      "  to/TO\n",
      "  its/PRP$\n",
      "  founding/NN\n",
      "  ideals/NNS\n",
      "  and/CC\n",
      "  carried/VBD\n",
      "  on/IN\n",
      "  a/DT\n",
      "  noble/JJ\n",
      "  dream/NN\n",
      "  ./.)\n",
      "subtree: (Chunk Mr./NNP Speaker/NNP)\n",
      "subtree: (Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "subtree: (Chunk Congress/NNP)\n",
      "subtree: (Chunk Supreme/NNP Court/NNP)\n",
      "subtree: (Chunk called/VBD America/NNP)\n",
      "2\n",
      "chunked: (S\n",
      "  Tonight/NN\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  comforted/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  hope/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  glad/JJ\n",
      "  reunion/NN\n",
      "  with/IN\n",
      "  the/DT\n",
      "  husband/NN\n",
      "  who/WP\n",
      "  was/VBD\n",
      "  taken/VBN\n",
      "  so/RB\n",
      "  long/RB\n",
      "  ago/RB\n",
      "  ,/,\n",
      "  and/CC\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  grateful/JJ\n",
      "  for/IN\n",
      "  the/DT\n",
      "  good/JJ\n",
      "  life/NN\n",
      "  of/IN\n",
      "  (Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "  ./.)\n",
      "subtree: (Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "3\n",
      "chunked: (S (/( (Chunk Applause/NNP) ./. )/))\n",
      "subtree: (Chunk Applause/NNP)\n",
      "4\n",
      "chunked: (S\n",
      "  (Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "  reacts/VBZ\n",
      "  to/TO\n",
      "  applause/VB\n",
      "  during/IN\n",
      "  his/PRP$\n",
      "  (Chunk State/NNP)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Union/NNP Address/NNP)\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (Chunk Capitol/NNP)\n",
      "  ,/,\n",
      "  (Chunk Tuesday/NNP)\n",
      "  ,/,\n",
      "  (Chunk Jan/NNP)\n",
      "  ./.)\n",
      "subtree: (Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "subtree: (Chunk State/NNP)\n",
      "subtree: (Chunk Union/NNP Address/NNP)\n",
      "subtree: (Chunk Capitol/NNP)\n",
      "subtree: (Chunk Tuesday/NNP)\n",
      "subtree: (Chunk Jan/NNP)\n"
     ]
    }
   ],
   "source": [
    "# chunking - group words into chunks based on some reg-ex\n",
    "# for reg-ex: https://pythonprogramming.net/regular-expressions-regex-tutorial-python-3/\n",
    "# noun-phrases: group nouns and related words (verbs, adverbs, adjectives)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "def get_chunks():\n",
    "    train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "    sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "    custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "    tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "    try:\n",
    "        for idx, i in enumerate(tokenized[:5]):\n",
    "            print(idx)\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            # 0 or more adverbs, verbs, nouns\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)           \n",
    "            print('chunked: {}'.format(chunked))\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print('subtree: {}'.format(subtree))\n",
    "            # doesn't work in jupyter\n",
    "            #chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "get_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "chunked: (S\n",
      "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP 'S/POS ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "  OF/IN\n",
      "  (Chunk\n",
      "    THE/NNP\n",
      "    UNION/NNP\n",
      "    January/NNP\n",
      "    31/CD\n",
      "    ,/,\n",
      "    2006/CD\n",
      "    THE/NNP\n",
      "    PRESIDENT/NNP\n",
      "    :/:\n",
      "    Thank/NNP\n",
      "    you/PRP)\n",
      "  all/DT\n",
      "  (Chunk ./.))\n",
      "1\n",
      "chunked: (S\n",
      "  (Chunk\n",
      "    Mr./NNP\n",
      "    Speaker/NNP\n",
      "    ,/,\n",
      "    Vice/NNP\n",
      "    President/NNP\n",
      "    Cheney/NNP\n",
      "    ,/,\n",
      "    members/NNS)\n",
      "  of/IN\n",
      "  (Chunk Congress/NNP ,/, members/NNS)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk\n",
      "    Supreme/NNP\n",
      "    Court/NNP\n",
      "    and/CC\n",
      "    diplomatic/JJ\n",
      "    corps/NN\n",
      "    ,/,\n",
      "    distinguished/JJ\n",
      "    guests/NNS\n",
      "    ,/,\n",
      "    and/CC\n",
      "    fellow/JJ\n",
      "    citizens/NNS\n",
      "    :/:)\n",
      "  Today/VB\n",
      "  (Chunk our/PRP$ nation/NN)\n",
      "  lost/VBD\n",
      "  a/DT\n",
      "  beloved/VBN\n",
      "  (Chunk ,/, graceful/JJ ,/, courageous/JJ woman/NN who/WP)\n",
      "  called/VBD\n",
      "  (Chunk America/NNP)\n",
      "  to/TO\n",
      "  (Chunk its/PRP$ founding/NN ideals/NNS and/CC)\n",
      "  carried/VBD\n",
      "  on/IN\n",
      "  a/DT\n",
      "  (Chunk noble/JJ dream/NN ./.))\n",
      "2\n",
      "chunked: (S\n",
      "  (Chunk Tonight/NN we/PRP)\n",
      "  are/VBP\n",
      "  comforted/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  (Chunk hope/NN)\n",
      "  of/IN\n",
      "  a/DT\n",
      "  (Chunk glad/JJ reunion/NN)\n",
      "  with/IN\n",
      "  the/DT\n",
      "  (Chunk husband/NN who/WP)\n",
      "  was/VBD\n",
      "  taken/VBN\n",
      "  (Chunk so/RB long/RB ago/RB ,/, and/CC we/PRP)\n",
      "  are/VBP\n",
      "  (Chunk grateful/JJ)\n",
      "  for/IN\n",
      "  the/DT\n",
      "  (Chunk good/JJ life/NN)\n",
      "  of/IN\n",
      "  (Chunk Coretta/NNP Scott/NNP King/NNP ./.))\n",
      "3\n",
      "chunked: (S (Chunk (/( Applause/NNP ./. )/)))\n",
      "4\n",
      "chunked: (S\n",
      "  (Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "  reacts/VBZ\n",
      "  to/TO\n",
      "  applause/VB\n",
      "  during/IN\n",
      "  (Chunk his/PRP$ State/NNP)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Union/NNP Address/NNP)\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (Chunk Capitol/NNP ,/, Tuesday/NNP ,/, Jan/NNP ./.))\n"
     ]
    }
   ],
   "source": [
    "# chinking - removing a chunk from a chunk\n",
    "# chink - chunk removed from a chunk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "def chink_chunk():\n",
    "    train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "    sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "    custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "    tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "    try:\n",
    "        for idx, i in enumerate(tokenized[:5]):\n",
    "            print(idx)\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            # expression b/w } { will be chinked (removed) from the chunk\n",
    "            # verbs, prepositions, determiners, or the word 'to' \n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print('chunked: {}'.format(chunked))\n",
    "            #chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "chink_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0\n",
      "namedEntT, bin=True: (S\n",
      "  PRESIDENT/NNP\n",
      "  (NE GEORGE/NNP)\n",
      "  W./NNP\n",
      "  BUSH/NNP\n",
      "  'S/POS\n",
      "  (NE ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  A/NNP\n",
      "  JOINT/NNP\n",
      "  SESSION/NNP\n",
      "  OF/IN\n",
      "  (NE THE/NNP)\n",
      "  (NE CONGRESS/NNP)\n",
      "  ON/NNP\n",
      "  THE/NNP\n",
      "  STATE/NNP\n",
      "  OF/IN\n",
      "  (NE THE/NNP UNION/NNP)\n",
      "  January/NNP\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  THE/NNP\n",
      "  PRESIDENT/NNP\n",
      "  :/:\n",
      "  Thank/NNP\n",
      "  you/PRP\n",
      "  all/DT\n",
      "  ./.)\n",
      "namedEntF, bin=True: (S\n",
      "  PRESIDENT/NNP\n",
      "  (PERSON GEORGE/NNP W./NNP BUSH/NNP)\n",
      "  'S/POS\n",
      "  (ORGANIZATION ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  A/NNP\n",
      "  (ORGANIZATION JOINT/NNP)\n",
      "  SESSION/NNP\n",
      "  OF/IN\n",
      "  (ORGANIZATION THE/NNP)\n",
      "  (ORGANIZATION CONGRESS/NNP)\n",
      "  ON/NNP\n",
      "  THE/NNP\n",
      "  (ORGANIZATION STATE/NNP OF/IN)\n",
      "  (ORGANIZATION THE/NNP)\n",
      "  (ORGANIZATION UNION/NNP)\n",
      "  January/NNP\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  (ORGANIZATION THE/NNP)\n",
      "  PRESIDENT/NNP\n",
      "  :/:\n",
      "  Thank/NNP\n",
      "  you/PRP\n",
      "  all/DT\n",
      "  ./.)\n",
      "idx: 1\n",
      "namedEntT, bin=True: (S\n",
      "  (NE Mr./NNP Speaker/NNP)\n",
      "  ,/,\n",
      "  Vice/NNP\n",
      "  President/NNP\n",
      "  (NE Cheney/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  (NE Congress/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (NE Supreme/NNP Court/NNP)\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:\n",
      "  Today/VB\n",
      "  our/PRP$\n",
      "  nation/NN\n",
      "  lost/VBD\n",
      "  a/DT\n",
      "  beloved/VBN\n",
      "  ,/,\n",
      "  graceful/JJ\n",
      "  ,/,\n",
      "  courageous/JJ\n",
      "  woman/NN\n",
      "  who/WP\n",
      "  called/VBD\n",
      "  (NE America/NNP)\n",
      "  to/TO\n",
      "  its/PRP$\n",
      "  founding/NN\n",
      "  ideals/NNS\n",
      "  and/CC\n",
      "  carried/VBD\n",
      "  on/IN\n",
      "  a/DT\n",
      "  noble/JJ\n",
      "  dream/NN\n",
      "  ./.)\n",
      "namedEntF, bin=True: (S\n",
      "  (PERSON Mr./NNP Speaker/NNP)\n",
      "  ,/,\n",
      "  Vice/NNP\n",
      "  President/NNP\n",
      "  (PERSON Cheney/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  (ORGANIZATION Congress/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Supreme/NNP Court/NNP)\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:\n",
      "  Today/VB\n",
      "  our/PRP$\n",
      "  nation/NN\n",
      "  lost/VBD\n",
      "  a/DT\n",
      "  beloved/VBN\n",
      "  ,/,\n",
      "  graceful/JJ\n",
      "  ,/,\n",
      "  courageous/JJ\n",
      "  woman/NN\n",
      "  who/WP\n",
      "  called/VBD\n",
      "  (GPE America/NNP)\n",
      "  to/TO\n",
      "  its/PRP$\n",
      "  founding/NN\n",
      "  ideals/NNS\n",
      "  and/CC\n",
      "  carried/VBD\n",
      "  on/IN\n",
      "  a/DT\n",
      "  noble/JJ\n",
      "  dream/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition: recognize \"entities\" like people, places, things, locations, \n",
    "# monetary figures, and more.\n",
    "# Two options with NLTK's named entity recognition: \n",
    "# (1) Recognize all named entities\n",
    "# (2) Recognize named entities as their respective type, like people, places, locations, etc.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "def get_ne():\n",
    "    train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "    sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "    custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "    tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "    try:\n",
    "        for idx, i in enumerate(tokenized[:2]):\n",
    "            print('idx: {}'.format(idx))\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEntT = nltk.ne_chunk(tagged, binary=True)\n",
    "            print('namedEntT, bin=True: {}'.format(namedEntT))\n",
    "            namedEntF = nltk.ne_chunk(tagged, binary=False)\n",
    "            print('namedEntF, bin=True: {}'.format(namedEntF))\n",
    "            #namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "get_ne()\n",
    "\n",
    "# NE Type and Examples - when binary=False\n",
    "# ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "# PERSON - Eddy Bonte, President Obama\n",
    "# LOCATION - Murray River, Mount Everest\n",
    "# DATE - June, 2008-06-29\n",
    "# TIME - two fifty a m, 1:30 p.m.\n",
    "# MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "# PERCENT - twenty pct, 18.75 %\n",
    "# FACILITY - Washington Monument, Stonehenge\n",
    "# GPE - South East Asia, Midlothian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "# lemmatizing - similar to stemming.\n",
    "# stemmed root word might not be in a dictionary (not always a real word), but lemmatized \n",
    "# words can always be found in a dictionary (real words).\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "# takes part of speech (pos) as argument\n",
    "# default = noun\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk location: /Users/faameem/anaconda/lib/python3.5/site-packages/nltk/__init__.py\n",
      "\n",
      "token #: 0\n",
      "[The King James Bible]\n",
      "\n",
      "The Old Testament of the King James Bible\n",
      "\n",
      "The First Book of Moses:  Called Genesis\n",
      "\n",
      "\n",
      "1:1 In the beginning God created the heaven and the earth.\n",
      "\n",
      "token #: 1\n",
      "1:2 And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep.\n",
      "\n",
      "token #: 2\n",
      "And the Spirit of God moved upon the face of the\n",
      "waters.\n",
      "\n",
      "token #: 3\n",
      "1:3 And God said, Let there be light: and there was light.\n",
      "\n",
      "token #: 4\n",
      "1:4 And God saw the light, that it was good: and God divided the light\n",
      "from the darkness.\n"
     ]
    }
   ],
   "source": [
    "# nltk corpora\n",
    "\n",
    "import nltk\n",
    "print('nltk location: {}'.format(nltk.__file__)) # shows location of nltk modules __init__.py\n",
    "\n",
    "# look into data.py for locations of nltk data\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "def get_corpora():\n",
    "    sample = gutenberg.raw(\"bible-kjv.txt\")\n",
    "    tok = sent_tokenize(sample)\n",
    "    for x in range(5):\n",
    "        print('\\ntoken #: {}\\n{}'.format(x,tok[x]))\n",
    "get_corpora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonym:\n",
      "plan.n.01\n",
      "synonym word:\n",
      "plan\n",
      "synonym definition:\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "synonym examples:\n",
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n",
      "\n",
      "good: synonyms:\n",
      "{'expert', 'salutary', 'soundly', 'adept', 'in_effect', 'just', 'goodness', 'serious', 'unspoilt', 'right', 'unspoiled', 'trade_good', 'skillful', 'honest', 'secure', 'honorable', 'well', 'proficient', 'sound', 'dependable', 'dear', 'commodity', 'safe', 'ripe', 'undecomposed', 'practiced', 'near', 'respectable', 'skilful', 'in_force', 'upright', 'thoroughly', 'estimable', 'full', 'effective', 'beneficial', 'good'}\n",
      "good: antonyms:\n",
      "{'badness', 'bad', 'ill', 'evil', 'evilness'}\n",
      "\n",
      "ship / boat similarity: 0.9090909090909091\n",
      "ship / car similarity: 0.6956521739130435\n",
      "ship / cat similarity: 0.32\n"
     ]
    }
   ],
   "source": [
    "# WordNet (https://wordnet.princeton.edu/) is a lexical database for the English language, \n",
    "# which was created by Princeton, and is part of the NLTK corpus.\n",
    "# You can use WordNet alongside the NLTK module to find the meanings of words, synonyms, \n",
    "# antonyms, and more.\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# synonym set\n",
    "syns = wordnet.synsets(\"program\")\n",
    "\n",
    "print('synonym:\\n{}'.format(syns[0].name()))\n",
    "print('synonym word:\\n{}'.format(syns[0].lemmas()[0].name()))\n",
    "print('synonym definition:\\n{}'.format(syns[0].definition()))\n",
    "print('synonym examples:\\n{}'.format(syns[0].examples()))\n",
    "\n",
    "#The lemmas will be synonyms, and then you can use .antonyms to find the antonyms to the lemmas. \n",
    "synonyms = []\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            for la in l.antonyms():\n",
    "                antonyms.append(la.name())\n",
    "                #antonyms.append(l.antonyms()[0].name())\n",
    "print('\\ngood: synonyms:\\n{}'.format(set(synonyms)))\n",
    "print('good: antonyms:\\n{}'.format(set(antonyms)))  \n",
    "\n",
    "\n",
    "# using WordNet to compare the similarity of two words and their tenses, \n",
    "# by employing the Wu and Palmer method for semantic related-ness.\n",
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "print('\\nship / boat similarity: {}'.format(w1.wup_similarity(w2)))\n",
    "\n",
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('car.n.01')\n",
    "print('ship / car similarity: {}'.format(w1.wup_similarity(w2)))\n",
    "\n",
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('cat.n.01')\n",
    "print('ship / cat similarity: {}'.format(w1.wup_similarity(w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n",
      "Classifier accuracy percent: 70.0\n",
      "Most Informative Features\n",
      "                    3000 = True              neg : pos    =     10.4 : 1.0\n",
      "              accessible = True              pos : neg    =      9.6 : 1.0\n",
      "                  doubts = True              pos : neg    =      8.9 : 1.0\n",
      "                 frances = True              pos : neg    =      8.9 : 1.0\n",
      "           unimaginative = True              neg : pos    =      8.4 : 1.0\n",
      "                 tribute = True              pos : neg    =      6.9 : 1.0\n",
      "                    jude = True              pos : neg    =      6.3 : 1.0\n",
      "                bothered = True              neg : pos    =      5.8 : 1.0\n",
      "                   plods = True              neg : pos    =      5.7 : 1.0\n",
      "                   vapid = True              neg : pos    =      5.7 : 1.0\n",
      "                    lame = True              neg : pos    =      5.7 : 1.0\n",
      "                 crowded = True              pos : neg    =      5.6 : 1.0\n",
      "                pfeiffer = True              pos : neg    =      5.6 : 1.0\n",
      "            subconscious = True              pos : neg    =      5.6 : 1.0\n",
      "               integrity = True              pos : neg    =      5.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "print(len(documents))\n",
    "random.shuffle(documents)\n",
    "\n",
    "#print(documents[1])\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "#print(all_words.most_common(15))\n",
    "#print(all_words[\"stupid\"])\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "#print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "#print(type(featuresets))\n",
    "#print(featuresets[0])\n",
    "print(len(featuresets))\n",
    "training_set = featuresets[:1900]\n",
    "testing_set = featuresets[1900:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "import pickle\n",
    "save_classifier = open(\"naivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_f = open(\"naivebayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 70.0\n",
      "Most Informative Features\n",
      "                    3000 = True              neg : pos    =     10.4 : 1.0\n",
      "              accessible = True              pos : neg    =      9.6 : 1.0\n",
      "                  doubts = True              pos : neg    =      8.9 : 1.0\n",
      "                 frances = True              pos : neg    =      8.9 : 1.0\n",
      "           unimaginative = True              neg : pos    =      8.4 : 1.0\n",
      "                 tribute = True              pos : neg    =      6.9 : 1.0\n",
      "                    jude = True              pos : neg    =      6.3 : 1.0\n",
      "                bothered = True              neg : pos    =      5.8 : 1.0\n",
      "                   plods = True              neg : pos    =      5.7 : 1.0\n",
      "                   vapid = True              neg : pos    =      5.7 : 1.0\n",
      "                    lame = True              neg : pos    =      5.7 : 1.0\n",
      "                 crowded = True              pos : neg    =      5.6 : 1.0\n",
      "                pfeiffer = True              pos : neg    =      5.6 : 1.0\n",
      "            subconscious = True              pos : neg    =      5.6 : 1.0\n",
      "               integrity = True              pos : neg    =      5.6 : 1.0\n",
      "MNB_classifier accuracy percent: 73.0\n",
      "BernoulliNB_classifier accuracy percent: 69.0\n",
      "LogisticRegression_classifier accuracy percent: 68.0\n",
      "SGDClassifier_classifier accuracy percent: 64.0\n",
      "SVC_classifier accuracy percent: 46.0\n",
      "LinearSVC_classifier accuracy percent: 63.0\n",
      "NuSVC_classifier accuracy percent: 65.0\n"
     ]
    }
   ],
   "source": [
    "# employing scikit-learn with nltk to get more algorithms\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", \\\n",
    "      (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", \\\n",
    "      (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", \\\n",
    "      (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", \\\n",
    "      (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", \\\n",
    "      (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", \\\n",
    "      (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", \\\n",
    "      (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", \\\n",
    "      (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voted_classifier accuracy percent: 67.0\n",
      "Classification: pos Confidence %: 57.14285714285714\n",
      "Classification: neg Confidence %: 85.71428571428571\n",
      "Classification: pos Confidence %: 100.0\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: pos Confidence %: 71.42857142857143\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: pos Confidence %: 57.14285714285714\n"
     ]
    }
   ],
   "source": [
    "# combining algorithms with nltk\n",
    "# combining classifier algorithms is done by creating a voting system, where each algorithm \n",
    "# gets one vote, and the classification that has the most votes is the chosen one.\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "        \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            #print('classifier: {}'.format(c))\n",
    "            v = c.classify(features)\n",
    "            #print('vote: {}'.format(v))\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        #print('choice_votes {}'.format(choice_votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "    \n",
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                  NuSVC_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  SGDClassifier_classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  LogisticRegression_classifier)\n",
    "\n",
    "#print(len(testing_set))\n",
    "print(\"voted_classifier accuracy percent:\", \\\n",
    "      (nltk.classify.accuracy(voted_classifier, testing_set))*100)\n",
    "\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[0][0]), \\\n",
    "      \"Confidence %:\",voted_classifier.confidence(testing_set[0][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[1][0]), \\\n",
    "      \"Confidence %:\",voted_classifier.confidence(testing_set[1][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[2][0]), \\\n",
    "      \"Confidence %:\",voted_classifier.confidence(testing_set[2][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[3][0]), \\\n",
    "      \"Confidence %:\",voted_classifier.confidence(testing_set[3][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[4][0]), \\\n",
    "      \"Confidence %:\",voted_classifier.confidence(testing_set[4][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[5][0]), \\\n",
    "      \"Confidence %:\",voted_classifier.confidence(testing_set[5][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[6][0]), \\\n",
    "      \"Confidence %:\",voted_classifier.confidence(testing_set[6][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[7][0]), \\\n",
    "      \"Confidence %:\",voted_classifier.confidence(testing_set[7][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[8][0]), \\\n",
    "      \"Confidence %:\",voted_classifier.confidence(testing_set[8][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[9][0]), \\\n",
    "      \"Confidence %:\",voted_classifier.confidence(testing_set[9][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[10][0]), \\\n",
    "      \"Confidence %:\",voted_classifier.confidence(testing_set[10][0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
